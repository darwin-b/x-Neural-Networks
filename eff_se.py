# -*- coding: utf-8 -*-
"""eff.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1czRED1rFJIqT84sT-XyEPEwNfyC8Qrnb

    If experiencing memory issues on google colab switch to Kaggle notebook and use gpu as accelerator.
"""

################################################################################
#
# LOGISTICS
#
#    Written By : Nikhil Darwin Bollepalli
#    
#
# FILE
#
#    eff_se.py 
#
#
# DESCRIPTION
#
#
#    A PyTorch implementation of the network described in section 3 of IR-Net_NetworksPaper.doc/pdf 
#    Trained in Google Colaboratory using a GPU instance
#    (Runtime - Change runtime type - Hardware accelerator - GPU)
#
#    design and training the network in table 1 of the paper with 
#    the SE enhanced building block in fig 2a 
#
#    output generated during training 
#    showing the per epoch statistics (for networks trained using standard network block as described in paper)
#    
#
#   Using 1 GPU(s)
  # Epoch   0 Time     32.9 lr = 0.002000 avg loss = 0.017915
  # Epoch   0 Time     31.4 lr = 0.002000 avg loss = 0.017757
  # Epoch   0 Time     31.6 lr = 0.002000 avg loss = 0.017608
  # Epoch   0 Time     31.8 lr = 0.002000 avg loss = 0.017473
  # Epoch   0 Time     31.9 lr = 0.002000 avg loss = 0.017352
  # Epoch   0 Time     31.9 lr = 0.002000 avg loss = 0.017237
  # Epoch   0 Time     31.8 lr = 0.002000 avg loss = 0.017109
  # Epoch   0 Time     31.9 lr = 0.002000 avg loss = 0.016977
  # Epoch   0 Time     31.8 lr = 0.002000 avg loss = 0.016855
  # Epoch   0 Time     31.8 lr = 0.002000 avg loss = 0.016725
  # Epoch   0 Time     31.6 lr = 0.002000 avg loss = 0.016610
  # Epoch   0 Time     31.9 lr = 0.002000 avg loss = 0.016500
  # Epoch   0 Time    407.9 lr = 0.002000 avg loss = 0.016433 accuracy = 12.46
  # Epoch   1 Time     58.3 lr = 0.041600 avg loss = 0.016576
  # Epoch   1 Time     31.5 lr = 0.041600 avg loss = 0.016099
  # Epoch   1 Time     31.2 lr = 0.041600 avg loss = 0.015811
  # Epoch   1 Time     31.3 lr = 0.041600 avg loss = 0.015574
  # Epoch   1 Time     31.5 lr = 0.041600 avg loss = 0.015385
  # Epoch   1 Time     31.6 lr = 0.041600 avg loss = 0.015225
  # Epoch   1 Time     31.4 lr = 0.041600 avg loss = 0.015073
  # Epoch   1 Time     31.3 lr = 0.041600 avg loss = 0.014929
  # Epoch   1 Time     31.4 lr = 0.041600 avg loss = 0.014805
  # Epoch   1 Time     31.4 lr = 0.041600 avg loss = 0.014674
  # Epoch   1 Time     31.3 lr = 0.041600 avg loss = 0.014568
  # Epoch   1 Time     31.4 lr = 0.041600 avg loss = 0.014447
  # Epoch   1 Time    403.6 lr = 0.041600 avg loss = 0.014381 accuracy = 20.40
  # Epoch   2 Time     58.4 lr = 0.081200 avg loss = 0.013416
  # Epoch   2 Time     31.3 lr = 0.081200 avg loss = 0.013294
  # Epoch   2 Time     31.5 lr = 0.081200 avg loss = 0.013200
  # Epoch   2 Time     31.5 lr = 0.081200 avg loss = 0.013065
  # Epoch   2 Time     31.5 lr = 0.081200 avg loss = 0.012954
  # Epoch   2 Time     31.6 lr = 0.081200 avg loss = 0.012845
  # Epoch   2 Time     31.3 lr = 0.081200 avg loss = 0.012753
  # Epoch   2 Time     31.5 lr = 0.081200 avg loss = 0.012645
  # Epoch   2 Time     31.3 lr = 0.081200 avg loss = 0.012552
  # Epoch   2 Time     31.2 lr = 0.081200 avg loss = 0.012466
  # Epoch   2 Time     31.2 lr = 0.081200 avg loss = 0.012375
  # Epoch   2 Time     31.2 lr = 0.081200 avg loss = 0.012282
  # Epoch   2 Time    403.1 lr = 0.081200 avg loss = 0.012224 accuracy = 29.78
  # Epoch   3 Time     58.0 lr = 0.120800 avg loss = 0.011494
  # Epoch   3 Time     31.3 lr = 0.120800 avg loss = 0.011445
  # Epoch   3 Time     31.4 lr = 0.120800 avg loss = 0.011358
  # Epoch   3 Time     31.3 lr = 0.120800 avg loss = 0.011256
  # Epoch   3 Time     31.2 lr = 0.120800 avg loss = 0.011170
  # Epoch   3 Time     31.2 lr = 0.120800 avg loss = 0.011087
  # Epoch   3 Time     31.4 lr = 0.120800 avg loss = 0.011008
  # Epoch   3 Time     31.3 lr = 0.120800 avg loss = 0.010935
  # Epoch   3 Time     31.3 lr = 0.120800 avg loss = 0.010869
  # Epoch   3 Time     31.1 lr = 0.120800 avg loss = 0.010815
  # Epoch   3 Time     31.3 lr = 0.120800 avg loss = 0.010769
  # Epoch   3 Time     31.2 lr = 0.120800 avg loss = 0.010709
  # Epoch   3 Time    401.8 lr = 0.120800 avg loss = 0.010670 accuracy = 31.24
  # Epoch   4 Time     57.8 lr = 0.160400 avg loss = 0.010227
  # Epoch   4 Time     31.1 lr = 0.160400 avg loss = 0.010178
  # Epoch   4 Time     31.5 lr = 0.160400 avg loss = 0.010116
  # Epoch   4 Time     31.5 lr = 0.160400 avg loss = 0.010101
  # Epoch   4 Time     31.6 lr = 0.160400 avg loss = 0.010061
  # Epoch   4 Time     31.5 lr = 0.160400 avg loss = 0.010011
  # Epoch   4 Time     31.4 lr = 0.160400 avg loss = 0.009975
  # Epoch   4 Time     31.4 lr = 0.160400 avg loss = 0.009928
  # Epoch   4 Time     31.4 lr = 0.160400 avg loss = 0.009883
  # Epoch   4 Time     31.4 lr = 0.160400 avg loss = 0.009831
  # Epoch   4 Time     31.5 lr = 0.160400 avg loss = 0.009790
  # Epoch   4 Time     31.5 lr = 0.160400 avg loss = 0.009761
  # Epoch   4 Time    403.9 lr = 0.160400 avg loss = 0.009735 accuracy = 40.88
  # Epoch   5 Time     58.8 lr = 0.200000 avg loss = 0.009334
  # Epoch   5 Time     31.6 lr = 0.200000 avg loss = 0.009360
  # Epoch   5 Time     31.5 lr = 0.200000 avg loss = 0.009352
  # Epoch   5 Time     31.6 lr = 0.200000 avg loss = 0.009311
  # Epoch   5 Time     31.7 lr = 0.200000 avg loss = 0.009288
  # Epoch   5 Time     31.7 lr = 0.200000 avg loss = 0.009253
  # Epoch   5 Time     31.5 lr = 0.200000 avg loss = 0.009232
  # Epoch   5 Time     31.6 lr = 0.200000 avg loss = 0.009220
  # Epoch   5 Time     31.6 lr = 0.200000 avg loss = 0.009177
  # Epoch   5 Time     31.7 lr = 0.200000 avg loss = 0.009150
  # Epoch   5 Time     31.5 lr = 0.200000 avg loss = 0.009110
  # Epoch   5 Time     31.5 lr = 0.200000 avg loss = 0.009088
  # Epoch   5 Time    405.9 lr = 0.200000 avg loss = 0.009071 accuracy = 37.66
  # Epoch   6 Time     58.3 lr = 0.199795 avg loss = 0.008672
  # Epoch   6 Time     31.5 lr = 0.199795 avg loss = 0.008613
  # Epoch   6 Time     31.5 lr = 0.199795 avg loss = 0.008583
  # Epoch   6 Time     31.5 lr = 0.199795 avg loss = 0.008562
  # Epoch   6 Time     31.5 lr = 0.199795 avg loss = 0.008555
  # Epoch   6 Time     31.5 lr = 0.199795 avg loss = 0.008539
  # Epoch   6 Time     31.3 lr = 0.199795 avg loss = 0.008517
  # Epoch   6 Time     31.3 lr = 0.199795 avg loss = 0.008498
  # Epoch   6 Time     31.6 lr = 0.199795 avg loss = 0.008483
  # Epoch   6 Time     31.5 lr = 0.199795 avg loss = 0.008474
  # Epoch   6 Time     31.4 lr = 0.199795 avg loss = 0.008455
  # Epoch   6 Time     31.5 lr = 0.199795 avg loss = 0.008439
  # Epoch   6 Time    404.5 lr = 0.199795 avg loss = 0.008434 accuracy = 47.98
  # Epoch   7 Time     58.2 lr = 0.199180 avg loss = 0.008054
  # Epoch   7 Time     31.4 lr = 0.199180 avg loss = 0.008000
  # Epoch   7 Time     31.5 lr = 0.199180 avg loss = 0.008015
  # Epoch   7 Time     31.4 lr = 0.199180 avg loss = 0.007979
  # Epoch   7 Time     31.5 lr = 0.199180 avg loss = 0.007973
  # Epoch   7 Time     31.4 lr = 0.199180 avg loss = 0.007982
  # Epoch   7 Time     31.5 lr = 0.199180 avg loss = 0.007975
  # Epoch   7 Time     31.7 lr = 0.199180 avg loss = 0.007985
  # Epoch   7 Time     31.7 lr = 0.199180 avg loss = 0.007983
  # Epoch   7 Time     31.8 lr = 0.199180 avg loss = 0.007977
  # Epoch   7 Time     31.5 lr = 0.199180 avg loss = 0.007962
  # Epoch   7 Time     31.5 lr = 0.199180 avg loss = 0.007954
  # Epoch   7 Time    405.0 lr = 0.199180 avg loss = 0.007945 accuracy = 47.02
  # Epoch   8 Time     58.5 lr = 0.198158 avg loss = 0.007695
  # Epoch   8 Time     31.8 lr = 0.198158 avg loss = 0.007673
  # Epoch   8 Time     31.5 lr = 0.198158 avg loss = 0.007687
  # Epoch   8 Time     31.8 lr = 0.198158 avg loss = 0.007671
  # Epoch   8 Time     31.8 lr = 0.198158 avg loss = 0.007649
  # Epoch   8 Time     31.7 lr = 0.198158 avg loss = 0.007623
  # Epoch   8 Time     31.7 lr = 0.198158 avg loss = 0.007620
  # Epoch   8 Time     31.6 lr = 0.198158 avg loss = 0.007606
  # Epoch   8 Time     31.5 lr = 0.198158 avg loss = 0.007600
  # Epoch   8 Time     31.6 lr = 0.198158 avg loss = 0.007597
  # Epoch   8 Time     31.6 lr = 0.198158 avg loss = 0.007593
  # Epoch   8 Time     31.4 lr = 0.198158 avg loss = 0.007569
  # Epoch   8 Time    406.4 lr = 0.198158 avg loss = 0.007570 accuracy = 52.30
  # Epoch   9 Time     58.3 lr = 0.196733 avg loss = 0.007204
  # Epoch   9 Time     31.6 lr = 0.196733 avg loss = 0.007231
  # Epoch   9 Time     31.4 lr = 0.196733 avg loss = 0.007243
  # Epoch   9 Time     31.5 lr = 0.196733 avg loss = 0.007261
  # Epoch   9 Time     31.4 lr = 0.196733 avg loss = 0.007256
  # Epoch   9 Time     31.6 lr = 0.196733 avg loss = 0.007249
  # Epoch   9 Time     31.8 lr = 0.196733 avg loss = 0.007261
  # Epoch   9 Time     31.7 lr = 0.196733 avg loss = 0.007263
  # Epoch   9 Time     31.7 lr = 0.196733 avg loss = 0.007264
  # Epoch   9 Time     31.8 lr = 0.196733 avg loss = 0.007253
  # Epoch   9 Time     31.7 lr = 0.196733 avg loss = 0.007249
  # Epoch   9 Time     31.6 lr = 0.196733 avg loss = 0.007242
  # Epoch   9 Time    406.5 lr = 0.196733 avg loss = 0.007247 accuracy = 50.98
  # Epoch  10 Time     58.7 lr = 0.194911 avg loss = 0.006895
  # Epoch  10 Time     31.6 lr = 0.194911 avg loss = 0.006943
  # Epoch  10 Time     31.7 lr = 0.194911 avg loss = 0.006968
  # Epoch  10 Time     31.7 lr = 0.194911 avg loss = 0.006986
  # Epoch  10 Time     31.7 lr = 0.194911 avg loss = 0.006992
  # Epoch  10 Time     31.7 lr = 0.194911 avg loss = 0.006996
  # Epoch  10 Time     31.5 lr = 0.194911 avg loss = 0.006986
  # Epoch  10 Time     31.6 lr = 0.194911 avg loss = 0.006986
  # Epoch  10 Time     31.7 lr = 0.194911 avg loss = 0.006985
  # Epoch  10 Time     31.6 lr = 0.194911 avg loss = 0.006983
  # Epoch  10 Time     31.8 lr = 0.194911 avg loss = 0.006983
  # Epoch  10 Time     31.7 lr = 0.194911 avg loss = 0.006976
  # Epoch  10 Time    407.3 lr = 0.194911 avg loss = 0.006980 accuracy = 52.56
  # Epoch  11 Time     59.0 lr = 0.192699 avg loss = 0.006809
  # Epoch  11 Time     31.8 lr = 0.192699 avg loss = 0.006759
  # Epoch  11 Time     31.7 lr = 0.192699 avg loss = 0.006774
  # Epoch  11 Time     31.4 lr = 0.192699 avg loss = 0.006792
  # Epoch  11 Time     31.5 lr = 0.192699 avg loss = 0.006780
  # Epoch  11 Time     31.6 lr = 0.192699 avg loss = 0.006789
  # Epoch  11 Time     31.4 lr = 0.192699 avg loss = 0.006780
  # Epoch  11 Time     31.5 lr = 0.192699 avg loss = 0.006788
  # Epoch  11 Time     31.4 lr = 0.192699 avg loss = 0.006783
  # Epoch  11 Time     31.5 lr = 0.192699 avg loss = 0.006785
  # Epoch  11 Time     31.4 lr = 0.192699 avg loss = 0.006777
  # Epoch  11 Time     31.4 lr = 0.192699 avg loss = 0.006775
  # Epoch  11 Time    405.3 lr = 0.192699 avg loss = 0.006771 accuracy = 57.32
  # Epoch  12 Time     58.4 lr = 0.190107 avg loss = 0.006601
  # Epoch  12 Time     31.5 lr = 0.190107 avg loss = 0.006563
  # Epoch  12 Time     31.5 lr = 0.190107 avg loss = 0.006585
  # Epoch  12 Time     31.7 lr = 0.190107 avg loss = 0.006604
  # Epoch  12 Time     31.5 lr = 0.190107 avg loss = 0.006574
  # Epoch  12 Time     31.5 lr = 0.190107 avg loss = 0.006564
  # Epoch  12 Time     31.5 lr = 0.190107 avg loss = 0.006563
  # Epoch  12 Time     31.5 lr = 0.190107 avg loss = 0.006558
  # Epoch  12 Time     31.5 lr = 0.190107 avg loss = 0.006564
  # Epoch  12 Time     30.8 lr = 0.190107 avg loss = 0.006548
  # Epoch  12 Time     30.8 lr = 0.190107 avg loss = 0.006556
  # Epoch  12 Time     30.8 lr = 0.190107 avg loss = 0.006557
  # Epoch  12 Time    402.1 lr = 0.190107 avg loss = 0.006556 accuracy = 57.40
  # Epoch  13 Time     56.9 lr = 0.187145 avg loss = 0.006311
  # Epoch  13 Time     30.8 lr = 0.187145 avg loss = 0.006334
  # Epoch  13 Time     30.7 lr = 0.187145 avg loss = 0.006316
  # Epoch  13 Time     30.6 lr = 0.187145 avg loss = 0.006341
  # Epoch  13 Time     30.8 lr = 0.187145 avg loss = 0.006339
  # Epoch  13 Time     30.8 lr = 0.187145 avg loss = 0.006354
  # Epoch  13 Time     30.6 lr = 0.187145 avg loss = 0.006365
  # Epoch  13 Time     30.8 lr = 0.187145 avg loss = 0.006381
  # Epoch  13 Time     30.7 lr = 0.187145 avg loss = 0.006382
  # Epoch  13 Time     30.8 lr = 0.187145 avg loss = 0.006390
  # Epoch  13 Time     31.0 lr = 0.187145 avg loss = 0.006384
  # Epoch  13 Time     31.4 lr = 0.187145 avg loss = 0.006380
  # Epoch  13 Time    395.9 lr = 0.187145 avg loss = 0.006385 accuracy = 55.20
  # Epoch  14 Time     57.4 lr = 0.183825 avg loss = 0.006127
  # Epoch  14 Time     30.8 lr = 0.183825 avg loss = 0.006186
  # Epoch  14 Time     30.8 lr = 0.183825 avg loss = 0.006213
  # Epoch  14 Time     30.8 lr = 0.183825 avg loss = 0.006200
  # Epoch  14 Time     30.8 lr = 0.183825 avg loss = 0.006230
  # Epoch  14 Time     30.9 lr = 0.183825 avg loss = 0.006244
  # Epoch  14 Time     30.9 lr = 0.183825 avg loss = 0.006246
  # Epoch  14 Time     30.8 lr = 0.183825 avg loss = 0.006240
  # Epoch  14 Time     30.9 lr = 0.183825 avg loss = 0.006241
  # Epoch  14 Time     30.8 lr = 0.183825 avg loss = 0.006249
  # Epoch  14 Time     30.8 lr = 0.183825 avg loss = 0.006242
  # Epoch  14 Time     30.9 lr = 0.183825 avg loss = 0.006249
  # Epoch  14 Time    396.6 lr = 0.183825 avg loss = 0.006245 accuracy = 58.56
  # Epoch  15 Time     57.6 lr = 0.180161 avg loss = 0.005880
  # Epoch  15 Time     30.9 lr = 0.180161 avg loss = 0.006009
  # Epoch  15 Time     31.1 lr = 0.180161 avg loss = 0.006037
  # Epoch  15 Time     30.8 lr = 0.180161 avg loss = 0.006057
  # Epoch  15 Time     30.8 lr = 0.180161 avg loss = 0.006047
  # Epoch  15 Time     30.7 lr = 0.180161 avg loss = 0.006053
  # Epoch  15 Time     30.9 lr = 0.180161 avg loss = 0.006056
  # Epoch  15 Time     30.9 lr = 0.180161 avg loss = 0.006066
  # Epoch  15 Time     30.8 lr = 0.180161 avg loss = 0.006058
  # Epoch  15 Time     30.8 lr = 0.180161 avg loss = 0.006069
  # Epoch  15 Time     30.8 lr = 0.180161 avg loss = 0.006072
  # Epoch  15 Time     30.8 lr = 0.180161 avg loss = 0.006086
  # Epoch  15 Time    396.7 lr = 0.180161 avg loss = 0.006090 accuracy = 55.42
  # Epoch  16 Time     57.4 lr = 0.176168 avg loss = 0.005772
  # Epoch  16 Time     30.9 lr = 0.176168 avg loss = 0.005850
  # Epoch  16 Time     30.9 lr = 0.176168 avg loss = 0.005864
  # Epoch  16 Time     31.0 lr = 0.176168 avg loss = 0.005879
  # Epoch  16 Time     30.9 lr = 0.176168 avg loss = 0.005895
  # Epoch  16 Time     30.8 lr = 0.176168 avg loss = 0.005904
  # Epoch  16 Time     30.8 lr = 0.176168 avg loss = 0.005896
  # Epoch  16 Time     31.2 lr = 0.176168 avg loss = 0.005898
  # Epoch  16 Time     30.8 lr = 0.176168 avg loss = 0.005896
  # Epoch  16 Time     30.8 lr = 0.176168 avg loss = 0.005904
  # Epoch  16 Time     30.9 lr = 0.176168 avg loss = 0.005921
  # Epoch  16 Time     30.9 lr = 0.176168 avg loss = 0.005926
  # Epoch  16 Time    397.6 lr = 0.176168 avg loss = 0.005923 accuracy = 59.42
  # Epoch  17 Time     58.0 lr = 0.171863 avg loss = 0.005760
  # Epoch  17 Time     30.8 lr = 0.171863 avg loss = 0.005759
  # Epoch  17 Time     31.0 lr = 0.171863 avg loss = 0.005769
  # Epoch  17 Time     31.1 lr = 0.171863 avg loss = 0.005806
  # Epoch  17 Time     31.0 lr = 0.171863 avg loss = 0.005799
  # Epoch  17 Time     30.9 lr = 0.171863 avg loss = 0.005821
  # Epoch  17 Time     30.9 lr = 0.171863 avg loss = 0.005811
  # Epoch  17 Time     31.0 lr = 0.171863 avg loss = 0.005826
  # Epoch  17 Time     31.1 lr = 0.171863 avg loss = 0.005833
  # Epoch  17 Time     31.1 lr = 0.171863 avg loss = 0.005832
  # Epoch  17 Time     31.0 lr = 0.171863 avg loss = 0.005840
  # Epoch  17 Time     31.1 lr = 0.171863 avg loss = 0.005843
  # Epoch  17 Time    398.7 lr = 0.171863 avg loss = 0.005846 accuracy = 59.68
  # Epoch  18 Time     57.4 lr = 0.167263 avg loss = 0.005625
  # Epoch  18 Time     31.0 lr = 0.167263 avg loss = 0.005703
  # Epoch  18 Time     31.0 lr = 0.167263 avg loss = 0.005691
  # Epoch  18 Time     31.1 lr = 0.167263 avg loss = 0.005702
  # Epoch  18 Time     30.9 lr = 0.167263 avg loss = 0.005689
  # Epoch  18 Time     31.0 lr = 0.167263 avg loss = 0.005705
  # Epoch  18 Time     31.2 lr = 0.167263 avg loss = 0.005703
  # Epoch  18 Time     31.0 lr = 0.167263 avg loss = 0.005714
  # Epoch  18 Time     31.0 lr = 0.167263 avg loss = 0.005709
  # Epoch  18 Time     30.9 lr = 0.167263 avg loss = 0.005726
  # Epoch  18 Time     31.1 lr = 0.167263 avg loss = 0.005730
  # Epoch  18 Time     31.0 lr = 0.167263 avg loss = 0.005737
  # Epoch  18 Time    398.8 lr = 0.167263 avg loss = 0.005732 accuracy = 58.82
  # Epoch  19 Time     57.7 lr = 0.162387 avg loss = 0.005500
  # Epoch  19 Time     31.0 lr = 0.162387 avg loss = 0.005504
  # Epoch  19 Time     31.1 lr = 0.162387 avg loss = 0.005531
  # Epoch  19 Time     31.0 lr = 0.162387 avg loss = 0.005539
  # Epoch  19 Time     31.2 lr = 0.162387 avg loss = 0.005550
  # Epoch  19 Time     31.1 lr = 0.162387 avg loss = 0.005548
  # Epoch  19 Time     31.3 lr = 0.162387 avg loss = 0.005555
  # Epoch  19 Time     31.3 lr = 0.162387 avg loss = 0.005564
  # Epoch  19 Time     31.0 lr = 0.162387 avg loss = 0.005571
  # Epoch  19 Time     31.2 lr = 0.162387 avg loss = 0.005580
  # Epoch  19 Time     31.2 lr = 0.162387 avg loss = 0.005583
  # Epoch  19 Time     31.0 lr = 0.162387 avg loss = 0.005595
  # Epoch  19 Time    399.9 lr = 0.162387 avg loss = 0.005597 accuracy = 61.20
  # Epoch  20 Time     57.9 lr = 0.157254 avg loss = 0.005413
  # Epoch  20 Time     31.1 lr = 0.157254 avg loss = 0.005401
  # Epoch  20 Time     31.2 lr = 0.157254 avg loss = 0.005362
  # Epoch  20 Time     31.2 lr = 0.157254 avg loss = 0.005399
  # Epoch  20 Time     31.0 lr = 0.157254 avg loss = 0.005404
  # Epoch  20 Time     31.0 lr = 0.157254 avg loss = 0.005407
  # Epoch  20 Time     31.2 lr = 0.157254 avg loss = 0.005424
  # Epoch  20 Time     31.1 lr = 0.157254 avg loss = 0.005445
  # Epoch  20 Time     31.1 lr = 0.157254 avg loss = 0.005448
  # Epoch  20 Time     31.1 lr = 0.157254 avg loss = 0.005455
  # Epoch  20 Time     31.1 lr = 0.157254 avg loss = 0.005460
  # Epoch  20 Time     31.1 lr = 0.157254 avg loss = 0.005469
  # Epoch  20 Time    400.0 lr = 0.157254 avg loss = 0.005476 accuracy = 62.76
  # Epoch  21 Time     57.9 lr = 0.151887 avg loss = 0.005202
  # Epoch  21 Time     31.0 lr = 0.151887 avg loss = 0.005302
  # Epoch  21 Time     31.2 lr = 0.151887 avg loss = 0.005309
  # Epoch  21 Time     31.2 lr = 0.151887 avg loss = 0.005337
  # Epoch  21 Time     31.1 lr = 0.151887 avg loss = 0.005348
  # Epoch  21 Time     31.2 lr = 0.151887 avg loss = 0.005359
  # Epoch  21 Time     31.3 lr = 0.151887 avg loss = 0.005361
  # Epoch  21 Time     31.2 lr = 0.151887 avg loss = 0.005367
  # Epoch  21 Time     31.1 lr = 0.151887 avg loss = 0.005378
  # Epoch  21 Time     31.4 lr = 0.151887 avg loss = 0.005378
  # Epoch  21 Time     31.2 lr = 0.151887 avg loss = 0.005375
  # Epoch  21 Time     31.3 lr = 0.151887 avg loss = 0.005380
  # Epoch  21 Time    400.9 lr = 0.151887 avg loss = 0.005383 accuracy = 61.14
  # Epoch  22 Time     57.8 lr = 0.146308 avg loss = 0.005143
  # Epoch  22 Time     31.2 lr = 0.146308 avg loss = 0.005130
  # Epoch  22 Time     31.1 lr = 0.146308 avg loss = 0.005125
  # Epoch  22 Time     31.2 lr = 0.146308 avg loss = 0.005142
  # Epoch  22 Time     31.4 lr = 0.146308 avg loss = 0.005170
  # Epoch  22 Time     31.2 lr = 0.146308 avg loss = 0.005193
  # Epoch  22 Time     31.3 lr = 0.146308 avg loss = 0.005212
  # Epoch  22 Time     31.3 lr = 0.146308 avg loss = 0.005215
  # Epoch  22 Time     31.2 lr = 0.146308 avg loss = 0.005228
  # Epoch  22 Time     31.4 lr = 0.146308 avg loss = 0.005241
  # Epoch  22 Time     31.3 lr = 0.146308 avg loss = 0.005254
  # Epoch  22 Time     31.3 lr = 0.146308 avg loss = 0.005268
  # Epoch  22 Time    402.1 lr = 0.146308 avg loss = 0.005266 accuracy = 63.44
  # Epoch  23 Time     58.3 lr = 0.140538 avg loss = 0.005055
  # Epoch  23 Time     31.5 lr = 0.140538 avg loss = 0.005105
  # Epoch  23 Time     31.1 lr = 0.140538 avg loss = 0.005114
  # Epoch  23 Time     31.3 lr = 0.140538 avg loss = 0.005121
  # Epoch  23 Time     31.4 lr = 0.140538 avg loss = 0.005135
  # Epoch  23 Time     31.5 lr = 0.140538 avg loss = 0.005153
  # Epoch  23 Time     31.3 lr = 0.140538 avg loss = 0.005152
  # Epoch  23 Time     31.3 lr = 0.140538 avg loss = 0.005155
  # Epoch  23 Time     31.4 lr = 0.140538 avg loss = 0.005152
  # Epoch  23 Time     31.4 lr = 0.140538 avg loss = 0.005156
  # Epoch  23 Time     31.4 lr = 0.140538 avg loss = 0.005161
  # Epoch  23 Time     31.5 lr = 0.140538 avg loss = 0.005162
  # Epoch  23 Time    403.1 lr = 0.140538 avg loss = 0.005158 accuracy = 63.50
  # Epoch  24 Time     58.2 lr = 0.134602 avg loss = 0.004940
  # Epoch  24 Time     31.3 lr = 0.134602 avg loss = 0.004984
  # Epoch  24 Time     31.4 lr = 0.134602 avg loss = 0.005020
  # Epoch  24 Time     31.5 lr = 0.134602 avg loss = 0.005018
  # Epoch  24 Time     31.4 lr = 0.134602 avg loss = 0.005026
  # Epoch  24 Time     31.3 lr = 0.134602 avg loss = 0.005033
  # Epoch  24 Time     31.4 lr = 0.134602 avg loss = 0.005048
  # Epoch  24 Time     31.3 lr = 0.134602 avg loss = 0.005062
  # Epoch  24 Time     31.4 lr = 0.134602 avg loss = 0.005076
  # Epoch  24 Time     31.2 lr = 0.134602 avg loss = 0.005066
  # Epoch  24 Time     31.4 lr = 0.134602 avg loss = 0.005067
  # Epoch  24 Time     31.3 lr = 0.134602 avg loss = 0.005074
  # Epoch  24 Time    403.3 lr = 0.134602 avg loss = 0.005074 accuracy = 64.10
  # Epoch  25 Time     58.3 lr = 0.128524 avg loss = 0.004893
  # Epoch  25 Time     31.4 lr = 0.128524 avg loss = 0.004862
  # Epoch  25 Time     31.3 lr = 0.128524 avg loss = 0.004878
  # Epoch  25 Time     31.5 lr = 0.128524 avg loss = 0.004885
  # Epoch  25 Time     31.5 lr = 0.128524 avg loss = 0.004899
  # Epoch  25 Time     31.5 lr = 0.128524 avg loss = 0.004924
  # Epoch  25 Time     31.4 lr = 0.128524 avg loss = 0.004938
  # Epoch  25 Time     31.5 lr = 0.128524 avg loss = 0.004935
  # Epoch  25 Time     31.4 lr = 0.128524 avg loss = 0.004932
  # Epoch  25 Time     31.6 lr = 0.128524 avg loss = 0.004941
  # Epoch  25 Time     31.3 lr = 0.128524 avg loss = 0.004942
  # Epoch  25 Time     31.3 lr = 0.128524 avg loss = 0.004952
  # Epoch  25 Time    403.9 lr = 0.128524 avg loss = 0.004952 accuracy = 62.66
  # Epoch  26 Time     58.3 lr = 0.122330 avg loss = 0.004806
  # Epoch  26 Time     31.4 lr = 0.122330 avg loss = 0.004793
  # Epoch  26 Time     31.4 lr = 0.122330 avg loss = 0.004791
  # Epoch  26 Time     31.4 lr = 0.122330 avg loss = 0.004801
  # Epoch  26 Time     31.5 lr = 0.122330 avg loss = 0.004795
  # Epoch  26 Time     31.4 lr = 0.122330 avg loss = 0.004806
  # Epoch  26 Time     31.4 lr = 0.122330 avg loss = 0.004825
  # Epoch  26 Time     31.5 lr = 0.122330 avg loss = 0.004844
  # Epoch  26 Time     31.4 lr = 0.122330 avg loss = 0.004844
  # Epoch  26 Time     31.3 lr = 0.122330 avg loss = 0.004856
  # Epoch  26 Time     31.4 lr = 0.122330 avg loss = 0.004862
  # Epoch  26 Time     31.5 lr = 0.122330 avg loss = 0.004863
  # Epoch  26 Time    404.1 lr = 0.122330 avg loss = 0.004865 accuracy = 65.32
  # Epoch  27 Time     58.4 lr = 0.116044 avg loss = 0.004710
  # Epoch  27 Time     31.3 lr = 0.116044 avg loss = 0.004755
  # Epoch  27 Time     31.2 lr = 0.116044 avg loss = 0.004726
  # Epoch  27 Time     31.5 lr = 0.116044 avg loss = 0.004710
  # Epoch  27 Time     31.4 lr = 0.116044 avg loss = 0.004736
  # Epoch  27 Time     31.5 lr = 0.116044 avg loss = 0.004730
  # Epoch  27 Time     31.5 lr = 0.116044 avg loss = 0.004730
  # Epoch  27 Time     31.4 lr = 0.116044 avg loss = 0.004740
  # Epoch  27 Time     31.4 lr = 0.116044 avg loss = 0.004739
  # Epoch  27 Time     31.5 lr = 0.116044 avg loss = 0.004745
  # Epoch  27 Time     31.4 lr = 0.116044 avg loss = 0.004748
  # Epoch  27 Time     31.5 lr = 0.116044 avg loss = 0.004755
  # Epoch  27 Time    403.8 lr = 0.116044 avg loss = 0.004758 accuracy = 66.26
  # Epoch  28 Time     58.4 lr = 0.109693 avg loss = 0.004566
  # Epoch  28 Time     31.4 lr = 0.109693 avg loss = 0.004620
  # Epoch  28 Time     31.3 lr = 0.109693 avg loss = 0.004600
  # Epoch  28 Time     31.5 lr = 0.109693 avg loss = 0.004587
  # Epoch  28 Time     31.4 lr = 0.109693 avg loss = 0.004614
  # Epoch  28 Time     31.2 lr = 0.109693 avg loss = 0.004626
  # Epoch  28 Time     31.5 lr = 0.109693 avg loss = 0.004624
  # Epoch  28 Time     31.4 lr = 0.109693 avg loss = 0.004638
  # Epoch  28 Time     31.4 lr = 0.109693 avg loss = 0.004635
  # Epoch  28 Time     31.4 lr = 0.109693 avg loss = 0.004642
  # Epoch  28 Time     31.4 lr = 0.109693 avg loss = 0.004646
  # Epoch  28 Time     31.5 lr = 0.109693 avg loss = 0.004659
  # Epoch  28 Time    403.9 lr = 0.109693 avg loss = 0.004667 accuracy = 63.98
  # Epoch  29 Time     58.6 lr = 0.103302 avg loss = 0.004522
  # Epoch  29 Time     31.7 lr = 0.103302 avg loss = 0.004539
  # Epoch  29 Time     31.5 lr = 0.103302 avg loss = 0.004533
  # Epoch  29 Time     31.7 lr = 0.103302 avg loss = 0.004541
  # Epoch  29 Time     31.4 lr = 0.103302 avg loss = 0.004531
  # Epoch  29 Time     31.5 lr = 0.103302 avg loss = 0.004529
  # Epoch  29 Time     31.6 lr = 0.103302 avg loss = 0.004546
  # Epoch  29 Time     31.4 lr = 0.103302 avg loss = 0.004540
  # Epoch  29 Time     31.5 lr = 0.103302 avg loss = 0.004532
  # Epoch  29 Time     31.5 lr = 0.103302 avg loss = 0.004526
  # Epoch  29 Time     31.5 lr = 0.103302 avg loss = 0.004539
  # Epoch  29 Time     31.4 lr = 0.103302 avg loss = 0.004547
  # Epoch  29 Time    405.6 lr = 0.103302 avg loss = 0.004542 accuracy = 64.26
  # Epoch  30 Time     58.8 lr = 0.096898 avg loss = 0.004402
  # Epoch  30 Time     31.6 lr = 0.096898 avg loss = 0.004438
  # Epoch  30 Time     31.7 lr = 0.096898 avg loss = 0.004431
  # Epoch  30 Time     31.4 lr = 0.096898 avg loss = 0.004437
  # Epoch  30 Time     31.6 lr = 0.096898 avg loss = 0.004446
  # Epoch  30 Time     31.6 lr = 0.096898 avg loss = 0.004439
  # Epoch  30 Time     31.8 lr = 0.096898 avg loss = 0.004434
  # Epoch  30 Time     31.5 lr = 0.096898 avg loss = 0.004441
  # Epoch  30 Time     31.5 lr = 0.096898 avg loss = 0.004443
  # Epoch  30 Time     31.5 lr = 0.096898 avg loss = 0.004452
  # Epoch  30 Time     31.5 lr = 0.096898 avg loss = 0.004450
  # Epoch  30 Time     31.6 lr = 0.096898 avg loss = 0.004458
  # Epoch  30 Time    406.1 lr = 0.096898 avg loss = 0.004463 accuracy = 67.46
  # Epoch  31 Time     58.8 lr = 0.090507 avg loss = 0.004235
  # Epoch  31 Time     31.5 lr = 0.090507 avg loss = 0.004229
  # Epoch  31 Time     31.5 lr = 0.090507 avg loss = 0.004314
  # Epoch  31 Time     31.6 lr = 0.090507 avg loss = 0.004288
  # Epoch  31 Time     31.6 lr = 0.090507 avg loss = 0.004294
  # Epoch  31 Time     31.6 lr = 0.090507 avg loss = 0.004301
  # Epoch  31 Time     31.5 lr = 0.090507 avg loss = 0.004310
  # Epoch  31 Time     31.6 lr = 0.090507 avg loss = 0.004314
  # Epoch  31 Time     31.6 lr = 0.090507 avg loss = 0.004322
  # Epoch  31 Time     31.5 lr = 0.090507 avg loss = 0.004336
  # Epoch  31 Time     31.5 lr = 0.090507 avg loss = 0.004339
  # Epoch  31 Time     31.5 lr = 0.090507 avg loss = 0.004350
  # Epoch  31 Time    405.9 lr = 0.090507 avg loss = 0.004351 accuracy = 68.82
  # Epoch  32 Time     58.8 lr = 0.084156 avg loss = 0.004262
  # Epoch  32 Time     31.5 lr = 0.084156 avg loss = 0.004224
  # Epoch  32 Time     31.4 lr = 0.084156 avg loss = 0.004220
  # Epoch  32 Time     31.4 lr = 0.084156 avg loss = 0.004229
  # Epoch  32 Time     31.6 lr = 0.084156 avg loss = 0.004226
  # Epoch  32 Time     31.4 lr = 0.084156 avg loss = 0.004235
  # Epoch  32 Time     31.6 lr = 0.084156 avg loss = 0.004252
  # Epoch  32 Time     31.7 lr = 0.084156 avg loss = 0.004249
  # Epoch  32 Time     31.9 lr = 0.084156 avg loss = 0.004244
  # Epoch  32 Time     31.2 lr = 0.084156 avg loss = 0.004249
  # Epoch  32 Time     31.6 lr = 0.084156 avg loss = 0.004250
  # Epoch  32 Time     31.5 lr = 0.084156 avg loss = 0.004262
  # Epoch  32 Time    405.2 lr = 0.084156 avg loss = 0.004268 accuracy = 66.18
  # Epoch  33 Time     58.8 lr = 0.077870 avg loss = 0.004103
  # Epoch  33 Time     31.4 lr = 0.077870 avg loss = 0.004114
  # Epoch  33 Time     31.6 lr = 0.077870 avg loss = 0.004122
  # Epoch  33 Time     31.5 lr = 0.077870 avg loss = 0.004114
  # Epoch  33 Time     31.6 lr = 0.077870 avg loss = 0.004113
  # Epoch  33 Time     31.7 lr = 0.077870 avg loss = 0.004117
  # Epoch  33 Time     31.8 lr = 0.077870 avg loss = 0.004121
  # Epoch  33 Time     31.6 lr = 0.077870 avg loss = 0.004132
  # Epoch  33 Time     31.5 lr = 0.077870 avg loss = 0.004144
  # Epoch  33 Time     31.8 lr = 0.077870 avg loss = 0.004148
  # Epoch  33 Time     31.6 lr = 0.077870 avg loss = 0.004145
  # Epoch  33 Time     31.6 lr = 0.077870 avg loss = 0.004149
  # Epoch  33 Time    406.3 lr = 0.077870 avg loss = 0.004152 accuracy = 69.02
  # Epoch  34 Time     59.2 lr = 0.071676 avg loss = 0.003950
  # Epoch  34 Time     31.6 lr = 0.071676 avg loss = 0.003935
  # Epoch  34 Time     31.5 lr = 0.071676 avg loss = 0.003984
  # Epoch  34 Time     31.6 lr = 0.071676 avg loss = 0.004009
  # Epoch  34 Time     31.7 lr = 0.071676 avg loss = 0.004011
  # Epoch  34 Time     31.5 lr = 0.071676 avg loss = 0.004012
  # Epoch  34 Time     31.5 lr = 0.071676 avg loss = 0.004014
  # Epoch  34 Time     31.5 lr = 0.071676 avg loss = 0.004018
  # Epoch  34 Time     31.4 lr = 0.071676 avg loss = 0.004031
  # Epoch  34 Time     31.5 lr = 0.071676 avg loss = 0.004031
  # Epoch  34 Time     31.5 lr = 0.071676 avg loss = 0.004038
  # Epoch  34 Time     31.6 lr = 0.071676 avg loss = 0.004052
  # Epoch  34 Time    406.5 lr = 0.071676 avg loss = 0.004052 accuracy = 69.02
  # Epoch  35 Time     59.0 lr = 0.065598 avg loss = 0.003858
  # Epoch  35 Time     31.8 lr = 0.065598 avg loss = 0.003833
  # Epoch  35 Time     31.5 lr = 0.065598 avg loss = 0.003834
  # Epoch  35 Time     31.7 lr = 0.065598 avg loss = 0.003849
  # Epoch  35 Time     31.7 lr = 0.065598 avg loss = 0.003850
  # Epoch  35 Time     31.6 lr = 0.065598 avg loss = 0.003870
  # Epoch  35 Time     31.5 lr = 0.065598 avg loss = 0.003880
  # Epoch  35 Time     31.4 lr = 0.065598 avg loss = 0.003888
  # Epoch  35 Time     31.4 lr = 0.065598 avg loss = 0.003887
  # Epoch  35 Time     31.5 lr = 0.065598 avg loss = 0.003909
  # Epoch  35 Time     31.5 lr = 0.065598 avg loss = 0.003918
  # Epoch  35 Time     31.4 lr = 0.065598 avg loss = 0.003929
  # Epoch  35 Time    405.7 lr = 0.065598 avg loss = 0.003933 accuracy = 67.24
  # Epoch  36 Time     58.5 lr = 0.059662 avg loss = 0.003806
  # Epoch  36 Time     31.7 lr = 0.059662 avg loss = 0.003802
  # Epoch  36 Time     31.5 lr = 0.059662 avg loss = 0.003811
  # Epoch  36 Time     31.7 lr = 0.059662 avg loss = 0.003816
  # Epoch  36 Time     31.4 lr = 0.059662 avg loss = 0.003814
  # Epoch  36 Time     31.3 lr = 0.059662 avg loss = 0.003818
  # Epoch  36 Time     31.3 lr = 0.059662 avg loss = 0.003810
  # Epoch  36 Time     31.3 lr = 0.059662 avg loss = 0.003813
  # Epoch  36 Time     31.6 lr = 0.059662 avg loss = 0.003817
  # Epoch  36 Time     31.3 lr = 0.059662 avg loss = 0.003820
  # Epoch  36 Time     31.4 lr = 0.059662 avg loss = 0.003823
  # Epoch  36 Time     31.3 lr = 0.059662 avg loss = 0.003831
  # Epoch  36 Time    404.2 lr = 0.059662 avg loss = 0.003837 accuracy = 69.40
  # Epoch  37 Time     58.3 lr = 0.053892 avg loss = 0.003751
  # Epoch  37 Time     31.5 lr = 0.053892 avg loss = 0.003702
  # Epoch  37 Time     31.5 lr = 0.053892 avg loss = 0.003661
  # Epoch  37 Time     31.7 lr = 0.053892 avg loss = 0.003678
  # Epoch  37 Time     31.2 lr = 0.053892 avg loss = 0.003682
  # Epoch  37 Time     31.3 lr = 0.053892 avg loss = 0.003679
  # Epoch  37 Time     31.4 lr = 0.053892 avg loss = 0.003689
  # Epoch  37 Time     31.3 lr = 0.053892 avg loss = 0.003685
  # Epoch  37 Time     31.3 lr = 0.053892 avg loss = 0.003680
  # Epoch  37 Time     31.5 lr = 0.053892 avg loss = 0.003681
  # Epoch  37 Time     31.4 lr = 0.053892 avg loss = 0.003684
  # Epoch  37 Time     31.3 lr = 0.053892 avg loss = 0.003694
  # Epoch  37 Time    404.0 lr = 0.053892 avg loss = 0.003706 accuracy = 69.04
  # Epoch  38 Time     58.5 lr = 0.048313 avg loss = 0.003563
  # Epoch  38 Time     31.4 lr = 0.048313 avg loss = 0.003557
  # Epoch  38 Time     31.5 lr = 0.048313 avg loss = 0.003582
  # Epoch  38 Time     31.3 lr = 0.048313 avg loss = 0.003592
  # Epoch  38 Time     31.3 lr = 0.048313 avg loss = 0.003596
  # Epoch  38 Time     31.5 lr = 0.048313 avg loss = 0.003585
  # Epoch  38 Time     31.5 lr = 0.048313 avg loss = 0.003600
  # Epoch  38 Time     31.5 lr = 0.048313 avg loss = 0.003617
  # Epoch  38 Time     31.4 lr = 0.048313 avg loss = 0.003603
  # Epoch  38 Time     31.4 lr = 0.048313 avg loss = 0.003601
  # Epoch  38 Time     31.5 lr = 0.048313 avg loss = 0.003607
  # Epoch  38 Time     31.4 lr = 0.048313 avg loss = 0.003614
  # Epoch  38 Time    404.0 lr = 0.048313 avg loss = 0.003627 accuracy = 70.60
  # Epoch  39 Time     58.4 lr = 0.042946 avg loss = 0.003408
  # Epoch  39 Time     31.6 lr = 0.042946 avg loss = 0.003427
  # Epoch  39 Time     31.5 lr = 0.042946 avg loss = 0.003440
  # Epoch  39 Time     31.4 lr = 0.042946 avg loss = 0.003474
  # Epoch  39 Time     31.6 lr = 0.042946 avg loss = 0.003496
  # Epoch  39 Time     31.5 lr = 0.042946 avg loss = 0.003493
  # Epoch  39 Time     31.5 lr = 0.042946 avg loss = 0.003482
  # Epoch  39 Time     31.4 lr = 0.042946 avg loss = 0.003485
  # Epoch  39 Time     31.4 lr = 0.042946 avg loss = 0.003491
  # Epoch  39 Time     31.4 lr = 0.042946 avg loss = 0.003501
  # Epoch  39 Time     31.4 lr = 0.042946 avg loss = 0.003501
  # Epoch  39 Time     31.4 lr = 0.042946 avg loss = 0.003498
  # Epoch  39 Time    404.7 lr = 0.042946 avg loss = 0.003507 accuracy = 69.32
  # Epoch  40 Time     58.3 lr = 0.037813 avg loss = 0.003398
  # Epoch  40 Time     31.3 lr = 0.037813 avg loss = 0.003401
  # Epoch  40 Time     31.6 lr = 0.037813 avg loss = 0.003391
  # Epoch  40 Time     31.3 lr = 0.037813 avg loss = 0.003379
  # Epoch  40 Time     31.5 lr = 0.037813 avg loss = 0.003367
  # Epoch  40 Time     31.5 lr = 0.037813 avg loss = 0.003342
  # Epoch  40 Time     31.4 lr = 0.037813 avg loss = 0.003345
  # Epoch  40 Time     31.4 lr = 0.037813 avg loss = 0.003354
  # Epoch  40 Time     31.4 lr = 0.037813 avg loss = 0.003369
  # Epoch  40 Time     31.5 lr = 0.037813 avg loss = 0.003371
  # Epoch  40 Time     31.4 lr = 0.037813 avg loss = 0.003369
  # Epoch  40 Time     31.4 lr = 0.037813 avg loss = 0.003379
  # Epoch  40 Time    404.2 lr = 0.037813 avg loss = 0.003380 accuracy = 71.36
  # Epoch  41 Time     58.8 lr = 0.032937 avg loss = 0.003228
  # Epoch  41 Time     31.7 lr = 0.032937 avg loss = 0.003272
  # Epoch  41 Time     31.3 lr = 0.032937 avg loss = 0.003278
  # Epoch  41 Time     31.4 lr = 0.032937 avg loss = 0.003256
  # Epoch  41 Time     31.6 lr = 0.032937 avg loss = 0.003271
  # Epoch  41 Time     31.3 lr = 0.032937 avg loss = 0.003280
  # Epoch  41 Time     31.4 lr = 0.032937 avg loss = 0.003289
  # Epoch  41 Time     31.3 lr = 0.032937 avg loss = 0.003286
  # Epoch  41 Time     31.6 lr = 0.032937 avg loss = 0.003289
  # Epoch  41 Time     31.4 lr = 0.032937 avg loss = 0.003293
  # Epoch  41 Time     31.4 lr = 0.032937 avg loss = 0.003296
  # Epoch  41 Time     31.4 lr = 0.032937 avg loss = 0.003292
  # Epoch  41 Time    404.5 lr = 0.032937 avg loss = 0.003299 accuracy = 71.76
  # Epoch  42 Time     58.5 lr = 0.028337 avg loss = 0.003244
  # Epoch  42 Time     31.5 lr = 0.028337 avg loss = 0.003183
  # Epoch  42 Time     31.4 lr = 0.028337 avg loss = 0.003172
  # Epoch  42 Time     31.7 lr = 0.028337 avg loss = 0.003148
  # Epoch  42 Time     31.6 lr = 0.028337 avg loss = 0.003142
  # Epoch  42 Time     31.6 lr = 0.028337 avg loss = 0.003145
  # Epoch  42 Time     31.6 lr = 0.028337 avg loss = 0.003159
  # Epoch  42 Time     31.6 lr = 0.028337 avg loss = 0.003166
  # Epoch  42 Time     31.7 lr = 0.028337 avg loss = 0.003171
  # Epoch  42 Time     31.5 lr = 0.028337 avg loss = 0.003176
  # Epoch  42 Time     31.4 lr = 0.028337 avg loss = 0.003177
  # Epoch  42 Time     31.4 lr = 0.028337 avg loss = 0.003172
  # Epoch  42 Time    405.4 lr = 0.028337 avg loss = 0.003174 accuracy = 71.12
  # Epoch  43 Time     58.6 lr = 0.024032 avg loss = 0.003132
  # Epoch  43 Time     31.4 lr = 0.024032 avg loss = 0.003076
  # Epoch  43 Time     31.6 lr = 0.024032 avg loss = 0.003056
  # Epoch  43 Time     31.6 lr = 0.024032 avg loss = 0.003053
  # Epoch  43 Time     31.5 lr = 0.024032 avg loss = 0.003058
  # Epoch  43 Time     31.4 lr = 0.024032 avg loss = 0.003086
  # Epoch  43 Time     31.6 lr = 0.024032 avg loss = 0.003084
  # Epoch  43 Time     31.3 lr = 0.024032 avg loss = 0.003089
  # Epoch  43 Time     31.4 lr = 0.024032 avg loss = 0.003094
  # Epoch  43 Time     31.4 lr = 0.024032 avg loss = 0.003098
  # Epoch  43 Time     31.4 lr = 0.024032 avg loss = 0.003095
  # Epoch  43 Time     31.5 lr = 0.024032 avg loss = 0.003094
  # Epoch  43 Time    404.7 lr = 0.024032 avg loss = 0.003099 accuracy = 71.80
  # Epoch  44 Time     58.5 lr = 0.020039 avg loss = 0.003019
  # Epoch  44 Time     31.6 lr = 0.020039 avg loss = 0.003011
  # Epoch  44 Time     31.8 lr = 0.020039 avg loss = 0.002997
  # Epoch  44 Time     31.5 lr = 0.020039 avg loss = 0.002991
  # Epoch  44 Time     31.5 lr = 0.020039 avg loss = 0.003003
  # Epoch  44 Time     31.3 lr = 0.020039 avg loss = 0.002996
  # Epoch  44 Time     31.7 lr = 0.020039 avg loss = 0.002994
  # Epoch  44 Time     31.3 lr = 0.020039 avg loss = 0.002999
  # Epoch  44 Time     31.5 lr = 0.020039 avg loss = 0.003001
  # Epoch  44 Time     31.5 lr = 0.020039 avg loss = 0.002995
  # Epoch  44 Time     31.5 lr = 0.020039 avg loss = 0.003003
  # Epoch  44 Time     31.5 lr = 0.020039 avg loss = 0.003005
  # Epoch  44 Time    405.1 lr = 0.020039 avg loss = 0.003005 accuracy = 72.40
  # Epoch  45 Time     58.3 lr = 0.016375 avg loss = 0.002971
  # Epoch  45 Time     31.4 lr = 0.016375 avg loss = 0.002941
  # Epoch  45 Time     31.7 lr = 0.016375 avg loss = 0.002913
  # Epoch  45 Time     31.7 lr = 0.016375 avg loss = 0.002922
  # Epoch  45 Time     31.7 lr = 0.016375 avg loss = 0.002910
  # Epoch  45 Time     31.4 lr = 0.016375 avg loss = 0.002900
  # Epoch  45 Time     31.2 lr = 0.016375 avg loss = 0.002893
  # Epoch  45 Time     31.4 lr = 0.016375 avg loss = 0.002897
  # Epoch  45 Time     31.3 lr = 0.016375 avg loss = 0.002903
  # Epoch  45 Time     31.4 lr = 0.016375 avg loss = 0.002905
  # Epoch  45 Time     31.3 lr = 0.016375 avg loss = 0.002911
  # Epoch  45 Time     31.1 lr = 0.016375 avg loss = 0.002913
  # Epoch  45 Time    403.8 lr = 0.016375 avg loss = 0.002911 accuracy = 71.62
  # Epoch  46 Time     58.0 lr = 0.013055 avg loss = 0.002764
  # Epoch  46 Time     31.2 lr = 0.013055 avg loss = 0.002777
  # Epoch  46 Time     31.2 lr = 0.013055 avg loss = 0.002790
  # Epoch  46 Time     31.1 lr = 0.013055 avg loss = 0.002811
  # Epoch  46 Time     31.2 lr = 0.013055 avg loss = 0.002808
  # Epoch  46 Time     31.1 lr = 0.013055 avg loss = 0.002803
  # Epoch  46 Time     31.2 lr = 0.013055 avg loss = 0.002805
  # Epoch  46 Time     31.0 lr = 0.013055 avg loss = 0.002807
  # Epoch  46 Time     31.1 lr = 0.013055 avg loss = 0.002811
  # Epoch  46 Time     31.2 lr = 0.013055 avg loss = 0.002814
  # Epoch  46 Time     31.3 lr = 0.013055 avg loss = 0.002820
  # Epoch  46 Time     31.1 lr = 0.013055 avg loss = 0.002826
  # Epoch  46 Time    400.8 lr = 0.013055 avg loss = 0.002828 accuracy = 72.26
  # Epoch  47 Time     58.0 lr = 0.010093 avg loss = 0.002738
  # Epoch  47 Time     31.2 lr = 0.010093 avg loss = 0.002752
  # Epoch  47 Time     31.1 lr = 0.010093 avg loss = 0.002769
  # Epoch  47 Time     31.1 lr = 0.010093 avg loss = 0.002760
  # Epoch  47 Time     31.1 lr = 0.010093 avg loss = 0.002748
  # Epoch  47 Time     31.0 lr = 0.010093 avg loss = 0.002736
  # Epoch  47 Time     31.2 lr = 0.010093 avg loss = 0.002739
  # Epoch  47 Time     31.3 lr = 0.010093 avg loss = 0.002747
  # Epoch  47 Time     31.0 lr = 0.010093 avg loss = 0.002759
  # Epoch  47 Time     31.2 lr = 0.010093 avg loss = 0.002760
  # Epoch  47 Time     31.2 lr = 0.010093 avg loss = 0.002756
  # Epoch  47 Time     31.1 lr = 0.010093 avg loss = 0.002752
  # Epoch  47 Time    400.8 lr = 0.010093 avg loss = 0.002749 accuracy = 72.92
  # Epoch  48 Time     58.0 lr = 0.007501 avg loss = 0.002726
  # Epoch  48 Time     31.0 lr = 0.007501 avg loss = 0.002705
  # Epoch  48 Time     31.0 lr = 0.007501 avg loss = 0.002674
  # Epoch  48 Time     31.1 lr = 0.007501 avg loss = 0.002647
  # Epoch  48 Time     31.0 lr = 0.007501 avg loss = 0.002665
  # Epoch  48 Time     31.1 lr = 0.007501 avg loss = 0.002659
  # Epoch  48 Time     31.0 lr = 0.007501 avg loss = 0.002667
  # Epoch  48 Time     31.0 lr = 0.007501 avg loss = 0.002668
  # Epoch  48 Time     31.1 lr = 0.007501 avg loss = 0.002660
  # Epoch  48 Time     31.2 lr = 0.007501 avg loss = 0.002664
  # Epoch  48 Time     31.1 lr = 0.007501 avg loss = 0.002668
  # Epoch  48 Time     31.2 lr = 0.007501 avg loss = 0.002668
  # Epoch  48 Time    399.5 lr = 0.007501 avg loss = 0.002671 accuracy = 73.00
  # Epoch  49 Time     57.9 lr = 0.005289 avg loss = 0.002685
  # Epoch  49 Time     31.1 lr = 0.005289 avg loss = 0.002663
  # Epoch  49 Time     31.2 lr = 0.005289 avg loss = 0.002642
  # Epoch  49 Time     31.1 lr = 0.005289 avg loss = 0.002654
  # Epoch  49 Time     31.0 lr = 0.005289 avg loss = 0.002639
  # Epoch  49 Time     31.3 lr = 0.005289 avg loss = 0.002644
  # Epoch  49 Time     31.1 lr = 0.005289 avg loss = 0.002645
  # Epoch  49 Time     31.2 lr = 0.005289 avg loss = 0.002635
  # Epoch  49 Time     31.1 lr = 0.005289 avg loss = 0.002629
  # Epoch  49 Time     31.1 lr = 0.005289 avg loss = 0.002622
  # Epoch  49 Time     31.1 lr = 0.005289 avg loss = 0.002623
  # Epoch  49 Time     31.0 lr = 0.005289 avg loss = 0.002621
  # Epoch  49 Time    399.9 lr = 0.005289 avg loss = 0.002618 accuracy = 72.68
  # Epoch  50 Time     57.8 lr = 0.003467 avg loss = 0.002583
  # Epoch  50 Time     31.0 lr = 0.003467 avg loss = 0.002574
  # Epoch  50 Time     31.2 lr = 0.003467 avg loss = 0.002580
  # Epoch  50 Time     31.2 lr = 0.003467 avg loss = 0.002604
  # Epoch  50 Time     31.1 lr = 0.003467 avg loss = 0.002607
  # Epoch  50 Time     31.1 lr = 0.003467 avg loss = 0.002605
  # Epoch  50 Time     31.1 lr = 0.003467 avg loss = 0.002609
  # Epoch  50 Time     31.0 lr = 0.003467 avg loss = 0.002609
  # Epoch  50 Time     31.1 lr = 0.003467 avg loss = 0.002609
  # Epoch  50 Time     31.0 lr = 0.003467 avg loss = 0.002602
  # Epoch  50 Time     31.0 lr = 0.003467 avg loss = 0.002599
  # Epoch  50 Time     31.3 lr = 0.003467 avg loss = 0.002597
  # Epoch  50 Time    399.9 lr = 0.003467 avg loss = 0.002602 accuracy = 72.96
  # Epoch  51 Time     57.7 lr = 0.002042 avg loss = 0.002567
  # Epoch  51 Time     31.1 lr = 0.002042 avg loss = 0.002568
  # Epoch  51 Time     30.9 lr = 0.002042 avg loss = 0.002555
  # Epoch  51 Time     31.1 lr = 0.002042 avg loss = 0.002550
  # Epoch  51 Time     31.3 lr = 0.002042 avg loss = 0.002562
  # Epoch  51 Time     31.1 lr = 0.002042 avg loss = 0.002580
  # Epoch  51 Time     31.0 lr = 0.002042 avg loss = 0.002577
  # Epoch  51 Time     31.1 lr = 0.002042 avg loss = 0.002567
  # Epoch  51 Time     30.9 lr = 0.002042 avg loss = 0.002561
  # Epoch  51 Time     31.0 lr = 0.002042 avg loss = 0.002555
  # Epoch  51 Time     31.0 lr = 0.002042 avg loss = 0.002557
  # Epoch  51 Time     31.0 lr = 0.002042 avg loss = 0.002553
  # Epoch  51 Time    399.1 lr = 0.002042 avg loss = 0.002552 accuracy = 73.42
  # Epoch  52 Time     57.5 lr = 0.001020 avg loss = 0.002560
  # Epoch  52 Time     31.0 lr = 0.001020 avg loss = 0.002589
  # Epoch  52 Time     30.8 lr = 0.001020 avg loss = 0.002564
  # Epoch  52 Time     30.7 lr = 0.001020 avg loss = 0.002566
  # Epoch  52 Time     31.2 lr = 0.001020 avg loss = 0.002561
  # Epoch  52 Time     30.8 lr = 0.001020 avg loss = 0.002560
  # Epoch  52 Time     30.9 lr = 0.001020 avg loss = 0.002561
  # Epoch  52 Time     30.9 lr = 0.001020 avg loss = 0.002552
  # Epoch  52 Time     31.0 lr = 0.001020 avg loss = 0.002551
  # Epoch  52 Time     30.7 lr = 0.001020 avg loss = 0.002548
  # Epoch  52 Time     30.9 lr = 0.001020 avg loss = 0.002550
  # Epoch  52 Time     30.9 lr = 0.001020 avg loss = 0.002553
  # Epoch  52 Time    397.3 lr = 0.001020 avg loss = 0.002551 accuracy = 73.72
  # Epoch  53 Time     57.2 lr = 0.000405 avg loss = 0.002537
  # Epoch  53 Time     30.8 lr = 0.000405 avg loss = 0.002504
  # Epoch  53 Time     31.0 lr = 0.000405 avg loss = 0.002535
  # Epoch  53 Time     31.0 lr = 0.000405 avg loss = 0.002521
  # Epoch  53 Time     30.8 lr = 0.000405 avg loss = 0.002523
  # Epoch  53 Time     30.8 lr = 0.000405 avg loss = 0.002525
  # Epoch  53 Time     30.8 lr = 0.000405 avg loss = 0.002526
  # Epoch  53 Time     30.8 lr = 0.000405 avg loss = 0.002531
  # Epoch  53 Time     30.7 lr = 0.000405 avg loss = 0.002521
  # Epoch  53 Time     30.7 lr = 0.000405 avg loss = 0.002520
  # Epoch  53 Time     30.8 lr = 0.000405 avg loss = 0.002522
  # Epoch  53 Time     30.8 lr = 0.000405 avg loss = 0.002520
  # Epoch  53 Time    396.2 lr = 0.000405 avg loss = 0.002523 accuracy = 73.28
  # Epoch  54 Time     57.2 lr = 0.000200 avg loss = 0.002479
  # Epoch  54 Time     30.9 lr = 0.000200 avg loss = 0.002492
  # Epoch  54 Time     30.8 lr = 0.000200 avg loss = 0.002520
  # Epoch  54 Time     30.9 lr = 0.000200 avg loss = 0.002530
  # Epoch  54 Time     30.8 lr = 0.000200 avg loss = 0.002520
  # Epoch  54 Time     30.9 lr = 0.000200 avg loss = 0.002521
  # Epoch  54 Time     30.9 lr = 0.000200 avg loss = 0.002529
  # Epoch  54 Time     30.8 lr = 0.000200 avg loss = 0.002520
  # Epoch  54 Time     30.9 lr = 0.000200 avg loss = 0.002523
  # Epoch  54 Time     30.6 lr = 0.000200 avg loss = 0.002519
  # Epoch  54 Time     30.9 lr = 0.000200 avg loss = 0.002514
  # Epoch  54 Time     31.0 lr = 0.000200 avg loss = 0.002520
  # Epoch  54 Time    396.4 lr = 0.000200 avg loss = 0.002521 accuracy = 73.46
#
#
################################################################################

################################################################################
#
# IMPORT
#
################################################################################

# torch
import torch
import torch.nn       as     nn
import torch.optim    as     optim
from   torch.autograd import Function

# torch utils
import torchvision
import torchvision.transforms as transforms

# additional libraries
import os
import urllib.request
import zipfile
import time
import math
import numpy             as np
import matplotlib.pyplot as plt

################################################################################
#
# PARAMETERS
#
################################################################################

# data
DATA_DIR_1        = 'data'
DATA_DIR_2        = 'data/imagenet64'
DATA_DIR_TRAIN    = 'data/imagenet64/train'
DATA_DIR_TEST     = 'data/imagenet64/val'
DATA_FILE_TRAIN_1 = 'Train1.zip'
DATA_FILE_TRAIN_2 = 'Train2.zip'
DATA_FILE_TRAIN_3 = 'Train3.zip'
DATA_FILE_TRAIN_4 = 'Train4.zip'
DATA_FILE_TRAIN_5 = 'Train5.zip'
DATA_FILE_TEST_1  = 'Val1.zip'
DATA_URL_TRAIN_1  = 'https://github.com/darwin-b/Data/raw/main/Image_Net64/Train1.zip'
DATA_URL_TRAIN_2  = 'https://github.com/darwin-b/Data/raw/main/Image_Net64/Train2.zip'
DATA_URL_TRAIN_3  = 'https://github.com/darwin-b/Data/raw/main/Image_Net64/Train3.zip'
DATA_URL_TRAIN_4  = 'https://github.com/darwin-b/Data/raw/main/Image_Net64/Train4.zip'
DATA_URL_TRAIN_5  = 'https://github.com/darwin-b/Data/raw/main/Image_Net64/Train5.zip'
DATA_URL_TEST_1   = 'https://github.com/darwin-b/Data/raw/main/Image_Net64/Val1.zip'
DATA_BATCH_SIZE   = 256
DATA_NUM_WORKERS  = 4
DATA_NUM_CHANNELS = 3
DATA_NUM_CLASSES  = 100
DATA_RESIZE       = 64
DATA_CROP         = 56
DATA_MEAN         = (0.485, 0.456, 0.406)
DATA_STD_DEV      = (0.229, 0.224, 0.225)

# model
MODEL_LEVEL_1_BLOCKS   = 1 # used but ignored in model creation
MODEL_LEVEL_2_BLOCKS   = 1
MODEL_LEVEL_3_BLOCKS   = 1
MODEL_LEVEL_4_BLOCKS   = 2
MODEL_LEVEL_5_BLOCKS   = 3
MODEL_LEVEL_6_BLOCKS   = 4
MODEL_LEVEL_7_BLOCKS   = 5

MODEL_LEVEL_1_CHANNELS = 16
MODEL_LEVEL_2_CHANNELS = 24
MODEL_LEVEL_3_CHANNELS = 40
MODEL_LEVEL_4_CHANNELS = 80
MODEL_LEVEL_5_CHANNELS = 160
MODEL_LEVEL_6_CHANNELS = 320
MODEL_LEVEL_7_CHANNELS = 1280

# training
TRAIN_LR_MAX              = 0.2
TRAIN_LR_INIT_SCALE       = 0.01
TRAIN_LR_FINAL_SCALE      = 0.001
TRAIN_LR_INIT_EPOCHS      = 5
TRAIN_LR_FINAL_EPOCHS     = 50 # 100
TRAIN_NUM_EPOCHS          = TRAIN_LR_INIT_EPOCHS + TRAIN_LR_FINAL_EPOCHS
TRAIN_LR_INIT             = TRAIN_LR_MAX*TRAIN_LR_INIT_SCALE
TRAIN_LR_FINAL            = TRAIN_LR_MAX*TRAIN_LR_FINAL_SCALE
TRAIN_INTRA_EPOCH_DISPLAY = 10000

# file
FILE_NAME_CHECK      = 'EffNetStyleCheck.pt'
FILE_NAME_BEST       = 'EffNetStyleBest.pt'
FILE_SAVE            = True
FILE_LOAD            = False
FILE_EXTEND_TRAINING = False
FILE_NEW_OPTIMIZER   = False

################################################################################
#
# DATA
#
################################################################################

# create a local directory structure for data storage
if (os.path.exists(DATA_DIR_1) == False):
    os.mkdir(DATA_DIR_1)
if (os.path.exists(DATA_DIR_2) == False):
    os.mkdir(DATA_DIR_2)
if (os.path.exists(DATA_DIR_TRAIN) == False):
    os.mkdir(DATA_DIR_TRAIN)
if (os.path.exists(DATA_DIR_TEST) == False):
    os.mkdir(DATA_DIR_TEST)

# download data
if (os.path.exists(DATA_FILE_TRAIN_1) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_1, DATA_FILE_TRAIN_1)
if (os.path.exists(DATA_FILE_TRAIN_2) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_2, DATA_FILE_TRAIN_2)
if (os.path.exists(DATA_FILE_TRAIN_3) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_3, DATA_FILE_TRAIN_3)
if (os.path.exists(DATA_FILE_TRAIN_4) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_4, DATA_FILE_TRAIN_4)
if (os.path.exists(DATA_FILE_TRAIN_5) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_5, DATA_FILE_TRAIN_5)
if (os.path.exists(DATA_FILE_TEST_1) == False):
    urllib.request.urlretrieve(DATA_URL_TEST_1, DATA_FILE_TEST_1)

# extract data
with zipfile.ZipFile(DATA_FILE_TRAIN_1, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_2, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_3, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_4, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_5, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TEST_1, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TEST)

# transforms
transform_train = transforms.Compose([transforms.RandomResizedCrop(DATA_CROP), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])
transform_test  = transforms.Compose([transforms.Resize(DATA_RESIZE), transforms.CenterCrop(DATA_CROP), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])

# data sets
dataset_train = torchvision.datasets.ImageFolder(DATA_DIR_TRAIN, transform=transform_train)
dataset_test  = torchvision.datasets.ImageFolder(DATA_DIR_TEST,  transform=transform_test)

# data loader
dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=DATA_BATCH_SIZE, shuffle=True)
dataloader_test  = torch.utils.data.DataLoader(dataset_test,  batch_size=DATA_BATCH_SIZE, shuffle=False)

################################################################################
#
# NETWORK BUILDING BLOCK
#
################################################################################

# Squeeze and excitation block (used this building block logic directly in InvResblock)
class SE_Block(nn.Module):
    def __init__(self, channels, r=4):
        super().__init__()
        self.squeeze = nn.AdaptiveAvgPool2d(1)
        self.excitation = nn.Sequential(
            nn.Linear(channels, channels // r, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // r, channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        bs, channels, _, _ = x.shape
        y = self.squeeze(x).view(bs, channels)
        y = self.excitation(y).view(bs, channels, 1, 1)
        return x * y.expand_as(x)


# inverted residual block
class InvResBlock(nn.Module):

    # initialization
    def __init__(self, Ni, Ne, No, F, S):

        # parent initialization
        super(InvResBlock, self).__init__()
    
        #identity 
        if ((Ni == No) and (S==1)):
            self.id = True
            # self.conv0 = nn.Conv2d(Ni,No,(1,1),stride=(S,S) padding=(0,0), dilation=(1,1), groups=1, bias=false, padding_mode='zeros' )
            # self.bn0 = nn.BatchNorm2d(No, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        else:
            self.id = False
           

       # residual
        Pr = np.floor(F/2).astype(int)
        Pc = np.floor(F/2).astype(int)

        self.conv1 = nn.Conv2d(Ni,Ne,(1,1),stride=(1,1) ,padding=(0,0), dilation=(1,1), groups=1, bias=False, padding_mode='zeros' )
        self.bn1 = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.relu1 = nn.ReLU()

        self.conv2 = nn.Conv2d(Ne,Ne,(F,F),stride=(S,S) ,padding=(Pr,Pc), dilation=(1,1), groups=1, bias=False, padding_mode='zeros' )
        self.bn2 = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.relu2 = nn.ReLU()

        # squeeze and excitation
        self.squeeze = nn.AdaptiveAvgPool2d(1)
        self.excitation = nn.Sequential(nn.Linear(Ne, Ne // 4, bias=False),
                                            nn.ReLU(inplace=True),
                                            nn.Linear(Ne // 4, Ne, bias=False),
                                            nn.Sigmoid()
                                        )

        self.conv3 = nn.Conv2d(Ne,No,(1,1),stride=(1,1) ,padding=(0,0), dilation=(1,1), groups=1, bias=False, padding_mode='zeros' )
        self.bn3 = nn.BatchNorm2d(No, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.relu3 = nn.ReLU()

        self.relu0 = nn.ReLU()

    # forward path
    def forward(self, x):

        # mapping input x to output y for the inverted residual block in fig 2a of
        # the paper via connecting the operators defined in the initialization
        # and returns output y


        if (self.id == True):
            id = x
        else:
            id = 0

        # residual
        res = self.conv1(x)
        res = self.bn1(res)
        res = self.relu1(res)
        res = self.conv2(res)
        res = self.bn2(res)

        # squeeze and excite forward
        d1,c,_,_ = res.shape
        res1 = self.squeeze(res).view(d1, c)
        res1 = self.excitation(res1).view(d1, c, 1, 1)
        res = res*res1.expand_as(res)

        res = self.relu2(res)
        res = self.conv3(res)
        res = self.bn3(res)

        # sum
        y = id + res
        y = self.relu0(y)

        return y

################################################################################
#
# NETWORK
#
################################################################################

# define
class Model(nn.Module):

    # initialization
    # adding necessary parameters to the init function to create the model defined
    # in table 1 of the paper
    def __init__(self,
                 data_num_channels,
                 model_level_1_blocks, model_level_1_channels,
                 model_level_2_blocks, model_level_2_channels,
                 model_level_3_blocks, model_level_3_channels,
                 model_level_4_blocks, model_level_4_channels,
                 model_level_5_blocks, model_level_5_channels,
                 model_level_6_blocks, model_level_6_channels,
                 model_level_7_blocks, model_level_7_channels,
                 data_num_classes):  
                #  model_level_6_blocks, model_level_6_channels,
                #  model_level_7_blocks, model_level_7_channels,
                #  model_level_8_blocks, model_level_8_channels,


        # parent initialization
        super(Model, self).__init__()

        # creating all of the operators for the network defined in table 1 of the
        # paper using a combination of Python, standard PyTorch operators and
        # the previously defined InvResBlock class
        
        
        # encoder level 1 (3-16)
        self.enc_1 = nn.ModuleList()
        # ne0 = 4*data_num_channels
        self.enc_1.append(nn.Conv2d(data_num_channels, model_level_1_channels, (3,3), stride=(1,1), padding=(1, 1), dilation=(1, 1), bias=False, padding_mode='zeros'))
        self.enc_1.append(nn.BatchNorm2d(model_level_1_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))
        self.enc_1.append(nn.ReLU())

        # encoder level 2 (16-24)
        self.enc_2 = nn.ModuleList()
        ne2 = 4*model_level_1_channels
        self.enc_2.append(InvResBlock(model_level_1_channels, ne2, model_level_1_channels,3,1))
        self.enc_2.append(InvResBlock(model_level_1_channels, ne2, model_level_2_channels,3,1))

        # encoder level 3 (24-40)
        self.enc_3 = nn.ModuleList()
        ne3 = 4*model_level_2_channels
        self.enc_3.append(InvResBlock(model_level_2_channels, ne3, model_level_2_channels,3,1))
        self.enc_3.append(InvResBlock(model_level_2_channels, ne3, model_level_3_channels,3,2))

        # encoder level 4 (40-80)
        self.enc_4 = nn.ModuleList()
        ne4 = 4*model_level_3_channels
            # Repeat 2 times
        for n in range(model_level_4_blocks):
            self.enc_4.append(InvResBlock(model_level_3_channels, ne4, model_level_3_channels,3,1))
            # self.enc_4.append(InvResBlock(model_level_3_channels, ne4, model_level_3_channels,3,1))

        self.enc_4.append(InvResBlock(model_level_3_channels, ne4, model_level_4_channels,3,2))

        # encoder level 5 (80-160)
        self.enc_5 = nn.ModuleList()
        ne5 = 4*model_level_4_channels
            # Repeat 3 times
        for n in range(model_level_5_blocks):
            self.enc_5.append(InvResBlock(model_level_4_channels, ne5, model_level_4_channels,3,1))
            # self.enc_5.append(InvResBlock(model_level_4_channels, ne5, model_level_4_channels,3,1))
            # self.enc_5.append(InvResBlock(model_level_4_channels, ne5, model_level_4_channels,3,1))
        self.enc_5.append(InvResBlock(model_level_4_channels, ne5, model_level_5_channels,3,2))

        # encoder level 6 (160-320)
        self.enc_6 = nn.ModuleList()
        ne6 = 4*model_level_5_channels
            # Repeat 4 times
        for n in range(model_level_6_blocks):
            self.enc_6.append(InvResBlock(model_level_5_channels, ne6, model_level_5_channels,3,1))
            # self.enc_6.append(InvResBlock(model_level_5_channels, ne6, model_level_5_channels,3,1))
            # self.enc_6.append(InvResBlock(model_level_5_channels, ne6, model_level_5_channels,3,1))
            # self.enc_6.append(InvResBlock(model_level_5_channels, ne6, model_level_5_channels,3,1))
        self.enc_6.append(InvResBlock(model_level_5_channels, ne6, model_level_6_channels,3,2))

        # encoder level 7 (320-1280)
        self.enc_7 = nn.ModuleList()
        # ne0 = 4*data_num_channels
        self.enc_7.append(nn.Conv2d(model_level_6_channels, model_level_7_channels, (1,1), stride=(1,1), padding=(0, 0), dilation=(1, 1), bias=False, padding_mode='zeros'))
        self.enc_7.append(nn.BatchNorm2d(model_level_7_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))
        self.enc_7.append(nn.ReLU())


        # decoder
        self.dec = nn.ModuleList()
        self.dec.append(nn.AdaptiveAvgPool2d((1, 1)))
        self.dec.append(nn.Flatten())
        self.dec.append(nn.Linear(model_level_7_channels, data_num_classes, bias=True))


    # forward path
    def forward(self, x):

        # mapping input x to output y for the network defined in table 1 of the
        # paper via connecting the operators defined in the initialization
        # and returns output y
        

        # encoder level 1
        for layer in self.enc_1:
            x = layer(x)

        # encoder level 2
        for layer in self.enc_2:
            x = layer(x)

        # encoder level 3
        for layer in self.enc_3:
            x = layer(x)

        # encoder level 4
        for layer in self.enc_4:
            x = layer(x)

        # encoder level 5
        for layer in self.enc_5:
            x = layer(x)

        # encoder level 6
        for layer in self.enc_6:
            x = layer(x)
        
        # encoder level 7
        for layer in self.enc_7:
            x = layer(x)

        # decoder
        for layer in self.dec:
            x = layer(x)

        # return
        return x
        # return y

# create
# add necessary parameters to the init function to create the model defined
# in table 1 of the paper
model = Model(DATA_NUM_CHANNELS,
                 MODEL_LEVEL_1_BLOCKS, MODEL_LEVEL_1_CHANNELS,
                 MODEL_LEVEL_2_BLOCKS, MODEL_LEVEL_2_CHANNELS,
                 MODEL_LEVEL_3_BLOCKS, MODEL_LEVEL_3_CHANNELS,
                 MODEL_LEVEL_4_BLOCKS, MODEL_LEVEL_4_CHANNELS,
                 MODEL_LEVEL_5_BLOCKS, MODEL_LEVEL_5_CHANNELS,
                 MODEL_LEVEL_6_BLOCKS, MODEL_LEVEL_6_CHANNELS,
                 MODEL_LEVEL_7_BLOCKS, MODEL_LEVEL_7_CHANNELS,
                 DATA_NUM_CLASSES) 

# enable data parallelization for multi GPU systems
if (torch.cuda.device_count() > 1):
    model = nn.DataParallel(model)
print('Using {0:d} GPU(s)'.format(torch.cuda.device_count()), flush=True)

################################################################################
#
# ERROR AND OPTIMIZER
#
################################################################################

# error (softmax cross entropy)
criterion = nn.CrossEntropyLoss()

# learning rate schedule
def lr_schedule(epoch):

    # linear warmup followed by 1/2 wave cosine decay
    if epoch < TRAIN_LR_INIT_EPOCHS:
        lr = (TRAIN_LR_MAX - TRAIN_LR_INIT)*(float(epoch)/TRAIN_LR_INIT_EPOCHS) + TRAIN_LR_INIT
    else:
        lr = TRAIN_LR_FINAL + 0.5*(TRAIN_LR_MAX - TRAIN_LR_FINAL)*(1.0 + math.cos(((float(epoch) - TRAIN_LR_INIT_EPOCHS)/(TRAIN_LR_FINAL_EPOCHS - 1.0))*math.pi))

    return lr

# optimizer
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, dampening=0.0, weight_decay=5e-5, nesterov=True)

################################################################################
#
# TRAINING
#
################################################################################

# start epoch
start_epoch = 0

# specify the device as the GPU if present with fallback to the CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# transfer the network to the device
model.to(device)

# load the last checkpoint
if (FILE_LOAD == True):
    checkpoint = torch.load(FILE_NAME_CHECK)
    model.load_state_dict(checkpoint['model_state_dict'])
    if (FILE_NEW_OPTIMIZER == False):
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    if (FILE_EXTEND_TRAINING == False):
        start_epoch = checkpoint['epoch'] + 1

# initialize the epoch
accuracy_best      = 0
start_time_display = time.time()
start_time_epoch   = time.time()

# cycle through the epochs
for epoch in range(start_epoch, TRAIN_NUM_EPOCHS):

    # initialize epoch training
    model.train()
    training_loss = 0.0
    num_batches   = 0
    num_display   = 0

    # set the learning rate for the epoch
    for g in optimizer.param_groups:
        g['lr'] = lr_schedule(epoch)

    # cycle through the training data set
    for data in dataloader_train:

        # extract a batch of data and move it to the appropriate device
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        
        # forward pass, loss, backward pass and weight update
        outputs = model(inputs)
        loss    = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # update statistics
        training_loss = training_loss + loss.item()
        num_batches   = num_batches + 1
        num_display   = num_display + DATA_BATCH_SIZE

        # display intra epoch results
        if (num_display > TRAIN_INTRA_EPOCH_DISPLAY):
            num_display          = 0
            elapsed_time_display = time.time() - start_time_display
            start_time_display   = time.time()
            print('Epoch {0:3d} Time {1:8.1f} lr = {2:8.6f} avg loss = {3:8.6f}'.format(epoch, elapsed_time_display, lr_schedule(epoch), (training_loss / num_batches) / DATA_BATCH_SIZE), flush=True)

    # initialize epoch testing
    model.eval()
    test_correct = 0
    test_total   = 0

    # no weight update / no gradient needed
    with torch.no_grad():

        # cycle through the testing data set
        for data in dataloader_test:

            # extract a batch of data and move it to the appropriate device
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            # forward pass and prediction
            outputs      = model(inputs)
            _, predicted = torch.max(outputs.data, 1)

            # update test set statistics
            test_total   = test_total + labels.size(0)
            test_correct = test_correct + (predicted == labels).sum().item()

      # Epoch statistics
    elapsed_time_epoch = time.time() - start_time_epoch
    start_time_epoch   = time.time()
    print('Epoch {0:3d} Time {1:8.1f} lr = {2:8.6f} avg loss = {3:8.6f} accuracy = {4:5.2f}'.format(epoch, elapsed_time_epoch, lr_schedule(epoch), (training_loss/num_batches)/DATA_BATCH_SIZE, (100.0*test_correct/test_total)), flush=True)

    # save a checkpoint
    if (FILE_SAVE == True):
        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE_NAME_CHECK)

    # save the best model
    accuracy_epoch = 100.0 * test_correct / test_total
    if ((FILE_SAVE == True) and (accuracy_epoch >= accuracy_best)):
        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE_NAME_BEST)